# VLM Geometry Bench

A benchmark for evaluating Vision Language Models' ability to identify, count, and locate simple geometric shapes in synthetic images.

## Overview

VLM Geometry Bench tests VLMs on fundamental visual perception tasks using synthetic images with known ground truth:

- **Spot Counting** - Count the number of circular spots in an image
- **Pattern Recognition** - Classify spot arrangements (random, hexagonal, empty)
- **Localization** - List coordinates of all spots (experimental)
- **Size Estimation** - Estimate spot diameter in micrometers (experimental)
- **Defect Detection** - Identify missing spots in hexagonal patterns (experimental)

The test suite consists of 92 images across 6 classes generated by [imagegen](../imagegen).

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd vlm-geometry-bench

# Install with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

## Quick Start

### Using Local Ollama

```bash
# Make sure Ollama is running with a vision model
ollama pull llama3.2-vision

# Run evaluation
uv run vlm-geometry-bench \
    --backend ollama \
    --model llama3.2-vision:latest \
    --testsuite d:/python/imagegen/testsuite \
    --tasks COUNT,PATTERN
```

### Using OpenRouter

```bash
# Set your API key
export OPENROUTER_API_KEY="your-key-here"

# Run evaluation with GPT-4o
uv run vlm-geometry-bench \
    --backend openrouter \
    --model openai/gpt-4o \
    --testsuite d:/python/imagegen/testsuite \
    --tasks COUNT,PATTERN
```

### Quick Test (Limited Samples)

```bash
# Test with 5 samples from 2 classes
uv run vlm-geometry-bench \
    --samples 5 \
    --classes CTRL,USSS \
    --tasks COUNT
```

## CLI Options

```
Usage: vlm-geometry-bench [OPTIONS]

Options:
  --backend [ollama|openrouter]  API backend to use (default: ollama)
  --base-url TEXT                API base URL (auto-detected by default)
  --api-key TEXT                 API key (required for OpenRouter)
  --model TEXT                   Model name (default: llava:7b)
  --testsuite TEXT               Path to imagegen test suite directory
  --classes TEXT                 Comma-separated image classes to evaluate
  --tasks TEXT                   Tasks to run: COUNT,LOCATE,PATTERN,SIZE,DEFECT
  --shots [0|3|5]                Number of few-shot examples (default: 0)
  --samples INTEGER              Limit samples per class (default: all)
  --output TEXT                  Output directory for results
  --timeout INTEGER              Request timeout in seconds (default: 120)
  --tolerance INTEGER            Count tolerance for 'within N' metric
  --verbose, -v                  Enable verbose logging
  --help                         Show this message and exit
```

## Image Classes

| Class | Description | Pattern Type |
|-------|-------------|--------------|
| CTRL | Control images (empty, single spot) | EMPTY/SINGLE |
| USSS | Uniform Spots Same Size (random placement) | RANDOM |
| USDS | Uniform Spots Different Sizes (random placement) | RANDOM |
| HSFR | Hex Spots Fixed Rigid (perfect hexagonal grid) | HEXAGONAL |
| HSRP | Hex Spots Random Perturbation (perturbed hex grid) | HEXAGONAL |
| HSDN | Hex Spots Defects + Noise (hex grid with defects) | HEXAGONAL |

## Evaluation Tasks

| Task | Description | Primary Metric |
|------|-------------|----------------|
| COUNT | Count spots in the image | Exact match rate |
| PATTERN | Classify arrangement (RANDOM/HEXAGONAL/EMPTY/SINGLE) | Accuracy |
| LOCATE | List (x,y) coordinates of spots | Detection rate |
| SIZE | Estimate spot diameter in micrometers | Within tolerance rate |
| DEFECT | Detect missing/extra spots | Detection accuracy |

## Baseline Results

Evaluation on 92 images with COUNT and PATTERN tasks (0-shot):

### Overall Performance

| Model | Size | COUNT Exact | PATTERN Acc | Time |
|-------|------|-------------|-------------|------|
| llava:7b | 4.7 GB | 2.2% | 48.9% | 26s |
| llama3.2-vision | 7.8 GB | 6.5% | 66.3% | 159s |
| minicpm-v | 5.5 GB | 4.3% | 51.1% | 318s |
| granite3.2-vision:2b | 2.4 GB | ~1%* | 39.1% | 116s |
| qwen3-vl:8b | 6.1 GB | ~4%* | 58.8% | 671s |

*Low success rate due to response parsing issues

### Pattern Recognition by Type

| Model | EMPTY | SINGLE | RANDOM | HEXAGONAL |
|-------|-------|--------|--------|-----------|
| llava:7b | 50% | 50% | 97% | 0% |
| llama3.2-vision | 0% | 100% | 91% | 42% |
| minicpm-v | 100% | 100% | 100% | 0% |
| granite3.2-vision:2b | 100% | 0% | 78% | 0% |
| qwen3-vl:8b | 100% | 100% | 100% | 0% |

### Key Findings

1. **Counting is challenging**: All models struggle with exact spot counting, achieving only 2-7% exact match rates
2. **Random patterns are easier**: Models achieve 78-100% accuracy on RANDOM patterns
3. **Hexagonal patterns are difficult**: Most models fail to recognize hexagonal arrangements (0-42%)
4. **llama3.2-vision performs best overall**: Best hexagonal recognition (42%) and highest pattern accuracy (66.3%)
5. **Speed vs accuracy tradeoff**: llava:7b is fastest (26s) but has lowest accuracy; qwen3-vl is slowest (671s) with moderate accuracy

## Output Files

Results are saved to `results/<model>_<timestamp>/`:

```
results/llama3.2-vision_latest_20260127_135028/
├── metrics.json        # Full evaluation metrics
├── leaderboard.csv     # Summary row for comparison
└── raw_responses.json  # Per-sample VLM responses
```

### metrics.json Structure

```json
{
  "config": { "model": "...", "tasks": [...], ... },
  "usage": {
    "elapsed_seconds": 159.4,
    "total_requests": 184,
    "success_rate": 100.0,
    "input_tokens": 16100,
    "output_tokens": 612
  },
  "results_by_task": {
    "COUNT": { "exact_match_rate": 6.5, "mean_absolute_error": 12.5, ... },
    "PATTERN": { "accuracy": 66.3, "per_pattern_accuracy": {...}, ... }
  },
  "results_by_class": { ... }
}
```

## Running Tests

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=vlm_geometry_bench

# Run specific test file
uv run pytest tests/test_response_parser.py -v
```

## Architecture

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for detailed system architecture, data flow diagrams, and component descriptions.

## Generating Test Images

The test suite is generated by [imagegen](../imagegen):

```bash
cd ../imagegen
uv run gentestsuite testsuite-config.yaml --output-dir ./testsuite
```

This creates 92 images across all 6 classes with ground truth in `manifest.yaml` and spot coordinates in CSV files.

## Adding New Evaluations

### New Image Classes

1. Update imagegen's `testsuite-config.yaml`
2. Regenerate test suite
3. Add class name to `VALID_IMAGE_CLASSES` in `config.py`

### New Evaluation Tasks

1. Add task ID to `VALID_TASKS` in `config.py`
2. Add prompt template in `prompts.py`
3. Add parsing logic in `response_parser.py`
4. Add metrics calculation in `metrics.py`
5. Update `evaluator.py` to handle the new task

### New VLM Backends

1. Add backend name to valid backends in `config.py`
2. Implement endpoint logic in `vision_client.py`
3. Add authentication handling as needed

## License

MIT

## Related Projects

- [imagegen](../imagegen) - Synthetic image generator for test suite creation
- [salbench](../salbench) - Reference implementation for VLM evaluation framework
